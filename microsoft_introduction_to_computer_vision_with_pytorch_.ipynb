{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ddab51-bc37-41be-9176-ae35c936d85d",
   "metadata": {},
   "source": [
    "# Introduction to processing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2251904-8ad1-4e6e-b440-32d0d880b7fc",
   "metadata": {},
   "source": [
    "## Computer Vision. Images as Tensors\n",
    "\n",
    "Computer Vision (CV) is a field that studies how computers can gain some degree of understanding from digital images and/or video. *Understanding* in this definition has a rather broad meaning - it can range from being able to distinguish between a cat and a dog on the picture, to more complex tasks such as describing the image in natural language.\n",
    "\n",
    "The most common problems of computer vision include:\n",
    "\n",
    "* **Image Classification** is the simplest task, when we need to classify an image into one of many pre-defined categories, for example, distinguish a cat from a dog on a photograph, or recognize a handwritten digit.\n",
    "\n",
    "* **Object Detection** is a bit more difficult task, in which we need to find known objects on the picture and localize them, that is, return the **bounding box** for each of recognized objects.\n",
    "\n",
    "* **Segmentation** is similar to object detection, but instead of giving bounding box we need to return an exact pixel map outlining each of the recognized objects.  \n",
    "\n",
    "![An image showing how computer vision object detection can be performed with cats, dogs, and ducks.](images/2-image-data-1.png)\n",
    "\n",
    "Image taken from [CS224d Stanford Course](https://cs224d.stanford.edu/index.html)\n",
    "\n",
    "We’ll focus on **image classification** task, and how neural networks can be used to solve it. As with any other machine learning tasks, to train a model for classifying images we’ll need a labeled dataset, that is, a large number of images for each of the classes. \n",
    "\n",
    "\n",
    "## Images as Tensors\n",
    "\n",
    "Computer Vision works with Images. As you probably know, images consist of pixels, so they can be thought of as a rectangular collection (array) of pixels.\n",
    "\n",
    "In the first part of this tutorial, we will deal with handwritten digit recognition. We will use the MNIST dataset, which consists of grayscale images of handwritten digits, 28x28 pixels. Each image can be represented as 28x28 array, and elements of this array would denote intensity of corresponding pixel - either in the scale of range 0 to 1 (in which case floating point numbers are used), or 0 to 255 (integers). A popular python library called `numpy` is often used with computer vision tasks, because it allows to operate with multidimensional arrays effectively.\n",
    "\n",
    "To deal with color images, we need some way to represent colors. In most cases, we represent each pixel by 3 intensity values, corresponding to Red (R), Green (G) and Blue (B) components. This color encoding is called RGB, and thus color image of size $W\\times H$ will be represented as an array of size $3\\times H\\times W$ (sometimes the order of components might be different, but the idea is the same).\n",
    "\n",
    "![Grayscale Image](images/2-image-data-2.png) | ![RGB Image](images/2-image-data-3.png)\n",
    "------|------\n",
    "5x5 Grayscale Image | 5x5 Color (RGB) Image\n",
    "\n",
    "Using multi-dimensional arrays to represent images also has an advantage, because we can use an extra dimension to store a sequence of images. For example, to represent a video fragment consisting of 200 frames with 800x600 dimension, we may use the tensor of size 200x3x600x800.\n",
    "\n",
    "Multi-dimensional arrays are also called **tensors**. Usually, we refer to tensors when we speak about some neural network framework, such as PyTorch. The main difference between tensors in PyTorch and numpy arrays is that tensors support parallel operations on GPU, if it is available. Also, PyTorch offers additional functionality, such as automatic differentiation, when operating on tensors.  \n",
    "\n",
    "## Import packages and load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9e6b0-fb6c-4f7b-86b0-04e5826abfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be780d74",
   "metadata": {},
   "source": [
    "PyTorch has a [number of datasets](https://pytorch.org/vision/stable/datasets.html) available right from the library. Here we are using the well-known [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits, available through `torchvison.datasets.MNIST` in PyTorch. The dataset object returns the data in the form of Python Imagine Library (PIL) images, which we convert to tensors by passing a `transform=ToTensor()` parameter. \n",
    "\n",
    "When using your own notebooks, you can also experiment with the other built in datasets, in particular [FashionMNIST](https://pytorch.org/vision/stable/datasets.html#fashion-mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST(root=\"data\", train=True,\n",
    "                   transform=ToTensor(), download=True)\n",
    "test_data = MNIST(root=\"data\", train=False,\n",
    "                  transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0dc7c",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "Now that we have downloaded the dataset we can visualize some of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493735",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_plt_grid = 3\n",
    "fig, axs = plt.subplots(size_plt_grid, size_plt_grid)\n",
    "for i in range(size_plt_grid):\n",
    "    for j in range(size_plt_grid):\n",
    "        axs[i][j].imshow(train_data.data[i*size_plt_grid+j])\n",
    "        axs[i][j].set_title(train_data.targets[i*size_plt_grid+j].item())\n",
    "        axs[i][j].set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e0ab0",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "\n",
    "We have a total of 6000 training images and 1000 testing images. Its important to split out the data for training and testing. We also want to do some data exploration to get a better idea of what our data looks like\n",
    "\n",
    "Each sample is a tuple in the following structure:\n",
    " * First element is the actual image of a digit, represented by a tensor of shape 1x28x28\n",
    " * Second element is a **label** that specifies which digit is represented by the tensor. It is a tensor that contains a number from 0 to 9.\n",
    "\n",
    "`data_train` is a training dataset that we will use to train our model on. `data_test` is a smaller test dataset that we can use to verify our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761565f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training samples:\", len(train_data))\n",
    "print(\"Test samples:\", len(test_data))\n",
    "\n",
    "print(\"Tensor size:\", train_data[0][0].shape)\n",
    "print(\"First 10 digits are:\", [test_data[i][1] for i in range(10)])\n",
    "\n",
    "print(\"First element array:\", train_data.data[0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39760e",
   "metadata": {},
   "source": [
    "All pixel intensities of the images are represented by floating-point values in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min intensity value:\",train_data[0][0].min().item())\n",
    "print(\"Max intensity value:\",train_data[0][0].max().item())\n",
    "print(train_data[0][0].size())\n",
    "print(train_data.data[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f40ab",
   "metadata": {},
   "source": [
    "## Loading your own images\n",
    "\n",
    "In most of the practical applications, you would have your own images located on disk that you want to use to train your neural network. In this case, you need to load them into PyTorch tensors. \n",
    "\n",
    "One of the ways to do that is to use one of the Python libraries for image manipulation, such as *Open CV*, or *PIL/Pillow*, or *imageio*. Once you load your image into numpy array, you can easily convert it to tensors. \n",
    "\n",
    "> <font color=\"red\">It is important to make sure that all values are scaled to the range [0..1] before you pass them to a neural network - it is the usual convention for data preparation, and all default weight initializations in neural networks are designed to work with this range. </font>`ToTensor` transform that we have seen above automatically scales PIL/numpy images with integer pixel values into [0..1] range.\n",
    "\n",
    "Even better approach is to use functionality in **Torchvision** library, namely `ImageFolder`. It does all the preprocessing steps automatically, and also assigns labels to images according to the directory structure. We will see the example of using `ImageFolder` later in this course, once we start classifying our own cats and dogs images.\n",
    "\n",
    "> It is important to note that all images should be scaled to the same size. If your original images have different aspect ratios, you need to decide how to handle this scaling - either by cropping images, or by padding extra space.\n",
    "\n",
    "## Takeaway\n",
    "\n",
    "Neural networks work with tensors, and before training any models we need to convert our dataset into a set of tensors. This will be often required. We have loaded training and test datasets, and we are ready to start training our first neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbe149",
   "metadata": {},
   "source": [
    "# Training a simple dense neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20485375",
   "metadata": {},
   "source": [
    "## Training a dense neural network\n",
    "\n",
    "Let's focus on the problem of handwritten digit recognition. It is a classification problem, because for each input image we need to specify the class - which digit it is.\n",
    "\n",
    "In this unit, we start with the simplest possible approach for image classification - a fully-connected neural network (which is also called a *perceptron*). We will recap the way neural networks are defined in PyTorch, and how the training algorithm works. If you are familiar with those concepts - feel free to skip to the next unit, where we introduce Convolutional Neural Networks (CNNs).\n",
    "\n",
    "We use `pytorchcv` helper to load all data we have talked about in the previous unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e305871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/pytorchcv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Sequential, Flatten, ReLU, Linear, CrossEntropyLoss, BatchNorm1d\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b84246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypeparameters\n",
    "epoches = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_data = MNIST(root=\"data\", train=True,\n",
    "                       transform=ToTensor(), download=True)\n",
    "    test_data = MNIST(root=\"data\", train=False,\n",
    "                      transform=ToTensor(), download=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_data, test_data = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ff6e0",
   "metadata": {},
   "source": [
    "## Fully-connected dense neural networks\n",
    "\n",
    "A basic **neural network** in PyTorch consists of a number of **layers**. The simplest network would include just one fully-connected layer, which is called **Linear** layer, with 784 inputs (one input for each pixel of the input image) and 10 outputs (one output for each class).\n",
    "\n",
    "<img alt=\"A graph showing how an image is broken into layers based on the pixels.\" src=\"images/3-train-dense-neural-networks-1.png\" width=\"60%\"/>\n",
    "\n",
    "As we discussed above, the dimension of our digit images is $1\\times28\\times28$, i.e. each image contains $28\\times28=784$ different pixels. Because linear layer expects its input as one-dimensional vector, we need to insert another layer into the network, called **Flatten**, to change input tensor shape from $1\\times28\\times28$ to $784$.\n",
    "\n",
    "After `Flatten`, there is a main linear layer (called `Dense` in PyTorch terminology) that converts 784 inputs to 10 outputs - one per class. We want $n$-th output of the network to return the probability of the input digit being equal to $n$.\n",
    "\n",
    "Because the output of a fully-connected layer is not normalized to be between 0 and 1, it cannot be thought of as probability. Moreover, if want outputs to be probabilities of different digits, they all need to add up to 1. To turn output vectors into probability vector, a function called **Softmax** is often used as the last activation function in a classification neural network. For example, $\\mathrm{softmax}([-1,1,2]) = [0.035,0.25,0.705]$.\n",
    "\n",
    "> In PyTorch, we often prefer to use **LogSoftmax** function, which will also compute logarithms of output probabilities. To turn the output vector into the actual probabilities, we need to take **torch.exp** of the output. \n",
    "\n",
    "Thus, the architecture of our network can be represented by the following sequence of layers:\n",
    "\n",
    "<img alt=\"An image showing the architecture of the network broken into a sequence of layers.\" src=\"images/3-train-dense-neural-networks-3.png\" width=\"90%\"/>\n",
    "\n",
    "It can be defined in PyTorch in the following way, using `Sequential` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear_relu_stack = Sequential(\n",
    "            Linear(in_features=28*28, out_features=250),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=250),\n",
    "            Linear(in_features=250, out_features=50),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=50),\n",
    "            Linear(in_features=50, out_features=10),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3722b8",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "To train the model we will need to create **batches** from our dataset of a certain size. PyTorch has an object called **DataLoader** that can create batches of our data for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790aa5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204411d",
   "metadata": {},
   "source": [
    "The training process steps are as follows:\n",
    "\n",
    "1. We take a minibatch from the input dataset, which consists of input data (features) and expected result (label).\n",
    "2. We calculate the predicted result for this minibatch. \n",
    "3. The difference between this result and expected result is calculated using a special function called the **loss function**. Loss function shows how different the output of the network is from the expected output. The goal of our training is to minimize the loss. \n",
    "4. We calculate the gradients of this loss function with respect to model weights (parameters), which are then used to adjust the weights to optimize the performance of the network. The amount of adjustment is controlled by a parameter called **learning rate**, and the details of optimization algorithm are defined in the **optimizer** object.\n",
    "5. We repeat those steps until the whole dataset is processed. One complete pass through the dataset is called **an epoch**. \n",
    "\n",
    "Here is a function that performs one epoch training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e886f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FullyConnectedNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    print(\"--------\")\n",
    "    size_train_data = len(train_dataloader.dataset)\n",
    "    for num_batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_float = loss.item()\n",
    "        if num_batch % 100 == 0:\n",
    "            current = num_batch*batch_size\n",
    "            print(\"loss:%f [%d/%d]\" % (loss_float, current, size_train_data))\n",
    "        if (num_batch % 10 == 0):\n",
    "            losses.append(loss_float)\n",
    "\n",
    "    size_test_data = len(test_dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        accuracy += (pred.argmax(dim=1) ==\n",
    "                     y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size_test_data\n",
    "    accuracy /= size_test_data\n",
    "    print(\"Test Error: \\n Accuracy: %f, Avg loss: %f \\n\" %\n",
    "          (accuracy, test_loss))\n",
    "\n",
    "plt.plot(np.squeeze(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FullyConnectedNetwork()\n",
    "model.load_state_dict(torch.load(\"microsoft_introduction_to_computer_vision_with_pytorch_fully_connected_network.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in test_dataloader:\n",
    "    y_pred=model(x)\n",
    "    y_pred=torch.softmax(y_pred,dim=1)\n",
    "    print(\"The prediction for the first batch of test dataset:\",y_pred.argmax(1))\n",
    "    print(\"The label for the first batch of test dataset:\",y)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa473f",
   "metadata": {},
   "source": [
    "# Use a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be058f40",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "In the previous unit we have learned how to define a multi-layered neural network using class definition, but those networks were generic, and not specialized for computer vision tasks. In this unit we will learn about **Convolutional Neural Networks (CNNs)**, which are specifically designed for computer vision.\n",
    "\n",
    "Computer vision is different from generic classification, because when we are trying to find a certain object in the picture, we are scanning the image looking for some specific **patterns** and their combinations. For example, when looking for a cat, we first may look for horizontal lines, which can form whiskers, and then certain combination of whiskers can tell us that it is actually a picture of a cat. Relative position and presence of certain patterns is important, and not their exact position on the image. \n",
    "\n",
    "To extract patterns, we will use the notion of **convolutional filters**. But first, let us load all dependencies and functions that we have defined in the previous units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, Flatten, Linear, ReLU, BatchNorm1d, CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST(root=\"data\", train=True,\n",
    "                   transform=ToTensor(), download=True)\n",
    "test_data = MNIST(root=\"data\", train=False,\n",
    "                  transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0dc7c",
   "metadata": {},
   "source": [
    "## Visualizing the dataset\n",
    "Now that we have downloaded the dataset we can visualize some of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493735",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_plt_grid = 3\n",
    "fig, axs = plt.subplots(size_plt_grid, size_plt_grid)\n",
    "for i in range(size_plt_grid):\n",
    "    for j in range(size_plt_grid):\n",
    "        axs[i][j].imshow(train_data.data[i*size_plt_grid+j])\n",
    "        axs[i][j].set_title(train_data.targets[i*size_plt_grid+j].item())\n",
    "        axs[i][j].set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf8baa",
   "metadata": {},
   "source": [
    "## Convolutional filters\n",
    "\n",
    "Convolutional filters are small windows that run over each pixel of the image and compute weighted average of the neighboring pixels.\n",
    "\n",
    "<img alt=\"Sliging window over 28x28 digit image\" src=\"images/4-convolutional-networks-1.png\" width=\"50%\"/>\n",
    "\n",
    "They are defined by matrices of weight coefficients. Let's see the examples of applying two different convolutional filters over our MNIST handwritten digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8957988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(image, filter):\n",
    "    with torch.no_grad():\n",
    "        filter_tensor = torch.tensor(filter).type(torch.float)\n",
    "        c = Conv2d(1, 1, (3, 3))\n",
    "        c.weight.copy_(filter_tensor)\n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        print(image.shape)\n",
    "        image_conv = c(image)\n",
    "        plt.imshow(image_conv[0][0])\n",
    "plot_convolution(train_data[0][0], [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9342dbc",
   "metadata": {},
   "source": [
    "First filter is called a **vertical edge filter**, and it is defined by the following matrix:\n",
    "$$\n",
    "\\left(\n",
    "    \\begin{matrix}\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "    \\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "When this filter goes over relatively uniform pixel field, all values add up to 0. However, when it encounters a vertical edge in the image, high spike value is generated. That's why in the images above you can see vertical edges represented by high and low values, while horizontal edges are averaged out.\n",
    "\n",
    "An opposite thing happens when we apply horizontal edge filter - horizontal lines are amplified, and vertical are averaged out.\n",
    "\n",
    "> If we apply $3\\times3$ filter to an image of size $28\\times28$ - the size of the image will become $26\\times26$, because the filter does not go over the image boundaries. In some cases, however, we may want to keep the size of the image the same, in which case image is padded with zeros on each side.\n",
    "\n",
    "In classical computer vision, multiple filters were applied to the image to generate features, which then were used by machine learning algorithm to build a classifier. However, in deep learning we construct networks that **learn** best convolutional filters to solve classification problem.\n",
    "\n",
    "To do that, we introduce **convolutional layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714bf26",
   "metadata": {},
   "source": [
    "## Covolutional layers\n",
    "\n",
    "Convolutional layers are defined using `nn.Conv2d` construction. We need to specify the following:\n",
    "* `in_channels` - number of input channels. In our case we are dealing with a grayscale image, thus number of input channels is 1. Color image has 3 channels (RGB).\n",
    "* `out_channels` - number of filters to use. We will use 9 different filters, which will give the network plenty of opportunities to explore which filters work best for our scenario.\n",
    "* `kernel_size` is the size of the sliding window. Usually 3x3 or 5x5 filters are used. The choice of filter size is usually chosen by experiment, that is by trying out different filter sizes and comparing resulting accuracy.\n",
    "\n",
    "Simplest CNN will contain one convolutional layer. Given the input size 28x28, after applying nine 5x5 filters we will end up with a tensor of 9x24x24 (the spatial size is smaller, because there are only 24 positions where a sliding interval of length 5 can fit into 28 pixels). Here the result of each filter is represented by a different channel in the image (thus the first dimension 9 corresponds to the number of filters).\n",
    "\n",
    "After convolution, we flatten 9x24x24 tensor into one vector of size 5184, and then add linear layer, to produce 10 classes. We also use `relu` activation function in between layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneConvolutionNetwork(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = Conv2d(in_channels=1, out_channels=9, kernel_size=(5, 5))\n",
    "        self.flatten = Flatten()\n",
    "        self.linear_relu_stack = Sequential(\n",
    "            Linear(in_features=9*24*24, out_features=1250),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=1250),\n",
    "            Linear(in_features=1250, out_features=250),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=250),\n",
    "            Linear(in_features=250, out_features=50),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=50),\n",
    "            Linear(in_features=50, out_features=10),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f441765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneConvolutionNetwork().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epoches = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    print(\"--------\")\n",
    "    size_train_data = len(train_dataloader.dataset)\n",
    "    for num_batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_float = loss.item()\n",
    "        if num_batch % 100 == 0:\n",
    "            current = num_batch*batch_size\n",
    "            print(\"loss:%f [%d/%d]\" % (loss_float, current, size_train_data))\n",
    "        if (num_batch % 10 == 0):\n",
    "            losses.append(loss_float)\n",
    "\n",
    "    size_test_data = len(test_dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        accuracy += (pred.argmax(dim=1) ==\n",
    "                     y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size_test_data\n",
    "    accuracy /= size_test_data\n",
    "    print(\"Test Error: \\n Accuracy: %f, Avg loss: %f \\n\" %\n",
    "          (accuracy, test_loss))\n",
    "\n",
    "plt.plot(np.squeeze(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423211c9",
   "metadata": {},
   "source": [
    "# Train multi-layer convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8a7ae",
   "metadata": {},
   "source": [
    "## Multi-Layered CNNs\n",
    "\n",
    "In the previous unit we have learned about convolutional filters that can extract patterns from images. For our MNIST classifier we used 9 5x5 filters, resulting in 9x24x24 tensor.\n",
    "\n",
    "We can use the same idea of convolution to extract higher-level patterns in the image. For example, rounded edges of digits such as 8 and 9 can be composed from a number of smaller strokes. To recognize those patterns, we can build another layer of convolution filters on top of the result of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, Sequential, Flatten, Linear, ReLU, BatchNorm1d, BatchNorm2d, CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "epoches = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST(root=\"data\", train=True,\n",
    "                   transform=ToTensor(), download=True)\n",
    "test_data = MNIST(root=\"data\", train=False,\n",
    "                  transform=ToTensor(), download=True)\n",
    "\n",
    "print(train_data.data.shape)\n",
    "print(train_data.targets.shape)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a05bb3",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "First convolutional layers looks for primitive patterns, such as horizontal or vertical lines. Next level of convolutional layers on top of them look for higher-level patterns, such as primitive shapes. More convolutional layers can combine those shapes into some parts of the picture, up to the final object that we are trying to classify. This creates a hierarchy of extracted patterns.\n",
    "\n",
    "When doing so, we also need to apply one trick: reducing the spatial size of the image. Once we have detected there is a horizontal stoke within a sliding window, it is not so important at which exact pixel it occurred. Thus we can \"scale down\" the size of the image, which is done using one of the **pooling layers**:\n",
    "\n",
    " * **Average Pooling** takes a sliding window (for example, 2x2 pixels) and computes an average of values within the window\n",
    " * **Max Pooling** replaces the window with the maximum value. The idea behind max pooling is to detect a presence of a certain pattern within the sliding window.\n",
    "\n",
    "<img alt=\"Max Pooling\" src=\"images/5-multilayer-convolutions-1.png\" width=\"50%\"/>\n",
    "\n",
    "Thus, in a typical CNN there would be composed of several convolutional layers, with pooling layers in between them to decrease dimensions of the image. We would also increase the number of filters, because as patterns become more advanced - there are more possible interesting combinations that we need to be looking for.\n",
    "\n",
    "![An image showing several convolutional layers with pooling layers.](images/5-multilayer-convolutions-2.png)\n",
    "\n",
    "Because of decreasing spatial dimensions and increasing feature/filters dimensions, this architecture is also called **pyramid architecture**. \n",
    "\n",
    "In the next example, we will use two-layered CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314371ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCNN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_laer = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=16,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=16),\n",
    "            MaxPool2d((2, 2)),\n",
    "            Conv2d(in_channels=16, out_channels=256,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=256),\n",
    "            MaxPool2d((2, 2)),\n",
    "        )\n",
    "        self.fc_layer = Sequential(\n",
    "            Flatten(),\n",
    "            Linear(in_features=256*7*7, out_features=16*7*7),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=16*7*7),\n",
    "            Linear(in_features=16*7*7, out_features=7*7),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=7*7),\n",
    "            Linear(in_features=7*7, out_features=10),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_laer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36796db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff773746",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    print(\"--------\")\n",
    "    size_train_data = len(train_dataloader.dataset)\n",
    "    for num_batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_float = loss.item()\n",
    "        if num_batch % 100 == 0:\n",
    "            current = num_batch*batch_size\n",
    "            print(\"loss:%f [%d/%d]\" % (loss_float, current, size_train_data))\n",
    "        if (num_batch % 10 == 0):\n",
    "            losses.append(loss_float)\n",
    "\n",
    "    size_test_data = len(test_dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        accuracy += (pred.argmax(dim=1) ==\n",
    "                     y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size_test_data\n",
    "    accuracy /= size_test_data\n",
    "    print(\"Test Error: \\n Accuracy: %f, Avg loss: %f \\n\" %\n",
    "          (accuracy, test_loss))\n",
    "\n",
    "plt.plot(np.squeeze(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f325fd",
   "metadata": {},
   "source": [
    "What you should probably observe is that we are able to achieve higher accuracy, and much faster - just with 1 or 2 epochs. It means that sophisticated network architecture needs much fewer data to figure out what is going on, and to extract generic patterns from our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7438b1",
   "metadata": {},
   "source": [
    "## Playing with real images from the CIFAR-10 dataset\n",
    "\n",
    "While our handwritten digit recognition problem may seem like a toy problem, we are now ready to do something more serious. Let's explore more advanced dataset of pictures of different objects, called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). It contains 60k 32x32 color images, divided into 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module,Conv2d,BatchNorm2d,MaxPool2d,Sequential,Flatten,Linear,ReLU,BatchNorm1d,CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c686319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "epoches=1\n",
    "batch_size=32\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "train_data = CIFAR10(root=\"data\", train=True,\n",
    "                     transform=ToTensor(), download=True)\n",
    "test_data = CIFAR10(root=\"data\", train=False,\n",
    "                    transform=ToTensor(), download=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2857857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "print(train_data[0][0].shape)\n",
    "print(test_data[0][0].shape)\n",
    "\n",
    "classes = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "size_plt_grid = 3\n",
    "fig, axs = plt.subplots(size_plt_grid, size_plt_grid)\n",
    "for i in range(size_plt_grid):\n",
    "    for j in range(size_plt_grid):\n",
    "        img = np.transpose(train_data[i*size_plt_grid+j][0].numpy(), (1, 2, 0))\n",
    "        axs[i][j].imshow(img)\n",
    "        axs[i][j].set_title(classes[train_data[i*size_plt_grid+j][1]])\n",
    "        axs[i][j].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fedaa",
   "metadata": {},
   "source": [
    "A well-known architecture for CIFAR-10 is called [LeNet](https://en.wikipedia.org/wiki/LeNet), and has been proposed by *Yann LeCun*. It follows the same principles as we have outlined above. However, since all images are color, input tensor size is $3\\times32\\times32$, and the $5\\times5$ convolutional filter is applied across color dimension as well - meaning that the size of convolution kernel matrix is $3\\times5\\times5$. \n",
    "\n",
    "We also do one more simplification to this model - we do not use `log_softmax` as output activation function, and just return the output of last fully-connected layer. In this case we can just use `CrossEntropyLoss` loss function to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41614598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = Sequential(\n",
    "            Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5)),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=(2, 2)),\n",
    "            Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5)),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=(2, 2)),\n",
    "            Conv2d(in_channels=16, out_channels=120, kernel_size=(5, 5)),\n",
    "            ReLU()\n",
    "        )\n",
    "        self.fc_layer = Sequential(\n",
    "            Flatten(),\n",
    "            Linear(in_features=120, out_features=64),\n",
    "            ReLU(),\n",
    "            Linear(in_features=64, out_features=10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=self.fc_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f917eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a25bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    print(\"--------\")\n",
    "    size_train_data = len(train_dataloader.dataset)\n",
    "    for num_batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_float = loss.item()\n",
    "        if num_batch % 100 == 0:\n",
    "            current = num_batch*batch_size\n",
    "            print(\"loss:%f [%d/%d]\" % (loss_float, current, size_train_data))\n",
    "        if (num_batch % 10 == 0):\n",
    "            losses.append(loss_float)\n",
    "\n",
    "    size_test_data = len(test_dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        accuracy += (pred.argmax(dim=1) ==\n",
    "                     y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size_test_data\n",
    "    accuracy /= size_test_data\n",
    "    print(\"Test Error: \\n Accuracy: %f, Avg loss: %f \\n\" %\n",
    "          (accuracy, test_loss))\n",
    "\n",
    "plt.plot(np.squeeze(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd312e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "358f6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module,Conv2d,BatchNorm2d,MaxPool2d,Sequential,Flatten,Linear,ReLU,BatchNorm1d,CrossEntropyLoss,Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b53832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c686319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "epoches=8\n",
    "batch_size=16\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b385a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preparing data\n",
    "train_data = CIFAR10(root=\"data\", train=True,\n",
    "                     transform=ToTensor(), download=True)\n",
    "test_data = CIFAR10(root=\"data\", train=False,\n",
    "                    transform=ToTensor(), download=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnForCIFAR10(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = Sequential(\n",
    "            Conv2d(in_channels=3, out_channels=16,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=16),\n",
    "            MaxPool2d(kernel_size=(2, 2)),\n",
    "            Conv2d(in_channels=16, out_channels=32,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=32),\n",
    "            MaxPool2d(kernel_size=(2, 2)),\n",
    "            Conv2d(in_channels=32, out_channels=64,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=64),\n",
    "            MaxPool2d(kernel_size=(2, 2)),\n",
    "            Conv2d(in_channels=64, out_channels=128,\n",
    "                   kernel_size=(3, 3), padding=\"same\"),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(num_features=128),\n",
    "            MaxPool2d(kernel_size=(2, 2))   \n",
    "        )\n",
    "        self.fc_layer = Sequential(\n",
    "            Flatten(),\n",
    "            Linear(in_features=128*2*2, out_features=64*2*2),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=64*2*2),\n",
    "            Dropout(0.2),\n",
    "            Linear(in_features=64*2*2, out_features=32*2*2),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(num_features=32*2*2),\n",
    "            Dropout(0.2),\n",
    "            Linear(in_features=32*2*2,out_features=16*2*2),\n",
    "            ReLU(),\n",
    "            BatchNorm1d(16*2*2),\n",
    "            Dropout(0.2),\n",
    "            Linear(in_features=16*2*2,out_features=10),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f936e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d893aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnForCIFAR10().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bda1a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aacac49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82675f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "--------\n",
      "loss:2.415004 [0/50000]\n",
      "loss:2.193067 [1600/50000]\n",
      "loss:1.883291 [3200/50000]\n",
      "loss:1.693259 [4800/50000]\n",
      "loss:1.547602 [6400/50000]\n",
      "loss:2.367515 [8000/50000]\n",
      "loss:1.539446 [9600/50000]\n",
      "loss:1.667590 [11200/50000]\n",
      "loss:1.321186 [12800/50000]\n",
      "loss:1.978662 [14400/50000]\n",
      "loss:1.646582 [16000/50000]\n",
      "loss:1.689950 [17600/50000]\n",
      "loss:1.637684 [19200/50000]\n",
      "loss:2.052505 [20800/50000]\n",
      "loss:1.816969 [22400/50000]\n",
      "loss:1.308505 [24000/50000]\n",
      "loss:1.495007 [25600/50000]\n",
      "loss:1.461843 [27200/50000]\n",
      "loss:1.464757 [28800/50000]\n",
      "loss:1.064626 [30400/50000]\n",
      "loss:1.504477 [32000/50000]\n",
      "loss:1.833682 [33600/50000]\n",
      "loss:1.583310 [35200/50000]\n",
      "loss:1.025218 [36800/50000]\n",
      "loss:1.092842 [38400/50000]\n",
      "loss:1.334993 [40000/50000]\n",
      "loss:1.402697 [41600/50000]\n",
      "loss:1.195090 [43200/50000]\n",
      "loss:1.021237 [44800/50000]\n",
      "loss:1.318081 [46400/50000]\n",
      "loss:1.068658 [48000/50000]\n",
      "loss:1.280814 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.529900, Avg loss: 0.084296 \n",
      "\n",
      "Epoch 1\n",
      "--------\n",
      "loss:0.856026 [0/50000]\n",
      "loss:1.383194 [1600/50000]\n",
      "loss:1.369615 [3200/50000]\n",
      "loss:1.376505 [4800/50000]\n",
      "loss:1.175242 [6400/50000]\n",
      "loss:1.386179 [8000/50000]\n",
      "loss:0.954887 [9600/50000]\n",
      "loss:0.919752 [11200/50000]\n",
      "loss:0.977783 [12800/50000]\n",
      "loss:1.419755 [14400/50000]\n",
      "loss:1.453192 [16000/50000]\n",
      "loss:0.574514 [17600/50000]\n",
      "loss:1.121386 [19200/50000]\n",
      "loss:0.967620 [20800/50000]\n",
      "loss:0.695957 [22400/50000]\n",
      "loss:0.978442 [24000/50000]\n",
      "loss:1.154832 [25600/50000]\n",
      "loss:1.133670 [27200/50000]\n",
      "loss:1.264280 [28800/50000]\n",
      "loss:1.235656 [30400/50000]\n",
      "loss:1.520619 [32000/50000]\n",
      "loss:1.281745 [33600/50000]\n",
      "loss:1.119447 [35200/50000]\n",
      "loss:1.020530 [36800/50000]\n",
      "loss:1.536212 [38400/50000]\n",
      "loss:0.952145 [40000/50000]\n",
      "loss:1.521058 [41600/50000]\n",
      "loss:0.918299 [43200/50000]\n",
      "loss:1.095371 [44800/50000]\n",
      "loss:1.148707 [46400/50000]\n",
      "loss:0.866601 [48000/50000]\n",
      "loss:0.705164 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.612600, Avg loss: 0.070133 \n",
      "\n",
      "Epoch 2\n",
      "--------\n",
      "loss:1.061253 [0/50000]\n",
      "loss:1.363123 [1600/50000]\n",
      "loss:1.399354 [3200/50000]\n",
      "loss:1.227364 [4800/50000]\n",
      "loss:1.080937 [6400/50000]\n",
      "loss:1.367103 [8000/50000]\n",
      "loss:0.905351 [9600/50000]\n",
      "loss:0.788103 [11200/50000]\n",
      "loss:0.768054 [12800/50000]\n",
      "loss:1.372666 [14400/50000]\n",
      "loss:0.658757 [16000/50000]\n",
      "loss:1.010044 [17600/50000]\n",
      "loss:1.037746 [19200/50000]\n",
      "loss:1.001750 [20800/50000]\n",
      "loss:0.602143 [22400/50000]\n",
      "loss:0.887256 [24000/50000]\n",
      "loss:1.174460 [25600/50000]\n",
      "loss:1.196416 [27200/50000]\n",
      "loss:1.059236 [28800/50000]\n",
      "loss:0.910654 [30400/50000]\n",
      "loss:1.033469 [32000/50000]\n",
      "loss:1.196660 [33600/50000]\n",
      "loss:1.474805 [35200/50000]\n",
      "loss:1.285455 [36800/50000]\n",
      "loss:1.019426 [38400/50000]\n",
      "loss:1.000535 [40000/50000]\n",
      "loss:0.907941 [41600/50000]\n",
      "loss:1.350590 [43200/50000]\n",
      "loss:0.627774 [44800/50000]\n",
      "loss:1.822332 [46400/50000]\n",
      "loss:1.203300 [48000/50000]\n",
      "loss:1.207327 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.665200, Avg loss: 0.061936 \n",
      "\n",
      "Epoch 3\n",
      "--------\n",
      "loss:0.615002 [0/50000]\n",
      "loss:0.814467 [1600/50000]\n",
      "loss:0.760056 [3200/50000]\n",
      "loss:0.765490 [4800/50000]\n",
      "loss:0.764653 [6400/50000]\n",
      "loss:1.305033 [8000/50000]\n",
      "loss:0.932371 [9600/50000]\n",
      "loss:1.310633 [11200/50000]\n",
      "loss:0.682160 [12800/50000]\n",
      "loss:0.483089 [14400/50000]\n",
      "loss:0.659367 [16000/50000]\n",
      "loss:0.670001 [17600/50000]\n",
      "loss:0.875639 [19200/50000]\n",
      "loss:1.029842 [20800/50000]\n",
      "loss:0.814344 [22400/50000]\n",
      "loss:0.915145 [24000/50000]\n",
      "loss:0.766762 [25600/50000]\n",
      "loss:0.687404 [27200/50000]\n",
      "loss:0.675772 [28800/50000]\n",
      "loss:0.436454 [30400/50000]\n",
      "loss:1.193537 [32000/50000]\n",
      "loss:0.665041 [33600/50000]\n",
      "loss:0.694654 [35200/50000]\n",
      "loss:0.688390 [36800/50000]\n",
      "loss:1.318288 [38400/50000]\n",
      "loss:1.519081 [40000/50000]\n",
      "loss:0.915944 [41600/50000]\n",
      "loss:1.054011 [43200/50000]\n",
      "loss:0.920781 [44800/50000]\n",
      "loss:0.814653 [46400/50000]\n",
      "loss:1.115427 [48000/50000]\n",
      "loss:1.212492 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.680200, Avg loss: 0.058758 \n",
      "\n",
      "Epoch 4\n",
      "--------\n",
      "loss:0.848963 [0/50000]\n",
      "loss:0.959475 [1600/50000]\n",
      "loss:0.647764 [3200/50000]\n",
      "loss:0.368730 [4800/50000]\n",
      "loss:0.943856 [6400/50000]\n",
      "loss:0.682187 [8000/50000]\n",
      "loss:0.641751 [9600/50000]\n",
      "loss:1.115345 [11200/50000]\n",
      "loss:0.505209 [12800/50000]\n",
      "loss:1.203190 [14400/50000]\n",
      "loss:0.511373 [16000/50000]\n",
      "loss:0.832423 [17600/50000]\n",
      "loss:0.791777 [19200/50000]\n",
      "loss:0.357518 [20800/50000]\n",
      "loss:0.885577 [22400/50000]\n",
      "loss:0.683969 [24000/50000]\n",
      "loss:0.642937 [25600/50000]\n",
      "loss:0.693545 [27200/50000]\n",
      "loss:0.626784 [28800/50000]\n",
      "loss:0.545218 [30400/50000]\n",
      "loss:0.755809 [32000/50000]\n",
      "loss:0.955524 [33600/50000]\n",
      "loss:0.971559 [35200/50000]\n",
      "loss:1.236583 [36800/50000]\n",
      "loss:0.801699 [38400/50000]\n",
      "loss:0.588738 [40000/50000]\n",
      "loss:1.030053 [41600/50000]\n",
      "loss:0.410242 [43200/50000]\n",
      "loss:0.473260 [44800/50000]\n",
      "loss:0.527639 [46400/50000]\n",
      "loss:0.824126 [48000/50000]\n",
      "loss:0.992541 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.684600, Avg loss: 0.058999 \n",
      "\n",
      "Epoch 5\n",
      "--------\n",
      "loss:1.044290 [0/50000]\n",
      "loss:1.093348 [1600/50000]\n",
      "loss:0.610943 [3200/50000]\n",
      "loss:0.567746 [4800/50000]\n",
      "loss:0.732167 [6400/50000]\n",
      "loss:1.083190 [8000/50000]\n",
      "loss:0.879191 [9600/50000]\n",
      "loss:0.529921 [11200/50000]\n",
      "loss:0.477570 [12800/50000]\n",
      "loss:0.473402 [14400/50000]\n",
      "loss:0.841321 [16000/50000]\n",
      "loss:0.586387 [17600/50000]\n",
      "loss:0.472297 [19200/50000]\n",
      "loss:0.825282 [20800/50000]\n",
      "loss:0.671801 [22400/50000]\n",
      "loss:0.630183 [24000/50000]\n",
      "loss:0.409123 [25600/50000]\n",
      "loss:0.460182 [27200/50000]\n",
      "loss:0.436311 [28800/50000]\n",
      "loss:0.666076 [30400/50000]\n",
      "loss:0.689758 [32000/50000]\n",
      "loss:0.502535 [33600/50000]\n",
      "loss:1.030300 [35200/50000]\n",
      "loss:0.785937 [36800/50000]\n",
      "loss:0.600724 [38400/50000]\n",
      "loss:1.037344 [40000/50000]\n",
      "loss:0.720124 [41600/50000]\n",
      "loss:0.774128 [43200/50000]\n",
      "loss:0.782331 [44800/50000]\n",
      "loss:0.569473 [46400/50000]\n",
      "loss:0.649856 [48000/50000]\n",
      "loss:0.902707 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.705300, Avg loss: 0.055397 \n",
      "\n",
      "Epoch 6\n",
      "--------\n",
      "loss:0.547779 [0/50000]\n",
      "loss:0.581513 [1600/50000]\n",
      "loss:0.578309 [3200/50000]\n",
      "loss:1.011076 [4800/50000]\n",
      "loss:0.525691 [6400/50000]\n",
      "loss:0.480728 [8000/50000]\n",
      "loss:0.579468 [9600/50000]\n",
      "loss:1.074893 [11200/50000]\n",
      "loss:0.718646 [12800/50000]\n",
      "loss:0.596807 [14400/50000]\n",
      "loss:0.998074 [16000/50000]\n",
      "loss:0.435933 [17600/50000]\n",
      "loss:0.508333 [19200/50000]\n",
      "loss:0.464210 [20800/50000]\n",
      "loss:1.086124 [22400/50000]\n",
      "loss:0.485313 [24000/50000]\n",
      "loss:0.460643 [25600/50000]\n",
      "loss:0.489926 [27200/50000]\n",
      "loss:1.146804 [28800/50000]\n",
      "loss:0.579144 [30400/50000]\n",
      "loss:0.640876 [32000/50000]\n",
      "loss:0.357032 [33600/50000]\n",
      "loss:0.611370 [35200/50000]\n",
      "loss:0.698803 [36800/50000]\n",
      "loss:0.918060 [38400/50000]\n",
      "loss:0.435655 [40000/50000]\n",
      "loss:1.124637 [41600/50000]\n",
      "loss:0.339968 [43200/50000]\n",
      "loss:0.398138 [44800/50000]\n",
      "loss:0.697365 [46400/50000]\n",
      "loss:0.633542 [48000/50000]\n",
      "loss:0.857346 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.711200, Avg loss: 0.055595 \n",
      "\n",
      "Epoch 7\n",
      "--------\n",
      "loss:0.608721 [0/50000]\n",
      "loss:0.535627 [1600/50000]\n",
      "loss:0.316280 [3200/50000]\n",
      "loss:0.408721 [4800/50000]\n",
      "loss:0.352960 [6400/50000]\n",
      "loss:0.859438 [8000/50000]\n",
      "loss:0.853333 [9600/50000]\n",
      "loss:0.885636 [11200/50000]\n",
      "loss:0.764181 [12800/50000]\n",
      "loss:0.465954 [14400/50000]\n",
      "loss:0.243766 [16000/50000]\n",
      "loss:0.315684 [17600/50000]\n",
      "loss:0.414318 [19200/50000]\n",
      "loss:0.433281 [20800/50000]\n",
      "loss:0.641958 [22400/50000]\n",
      "loss:0.254325 [24000/50000]\n",
      "loss:0.404969 [25600/50000]\n",
      "loss:1.113132 [27200/50000]\n",
      "loss:0.922793 [28800/50000]\n",
      "loss:0.744286 [30400/50000]\n",
      "loss:0.971819 [32000/50000]\n",
      "loss:0.871922 [33600/50000]\n",
      "loss:0.586957 [35200/50000]\n",
      "loss:0.301163 [36800/50000]\n",
      "loss:0.485228 [38400/50000]\n",
      "loss:0.355952 [40000/50000]\n",
      "loss:0.607305 [41600/50000]\n",
      "loss:0.514586 [43200/50000]\n",
      "loss:0.604499 [44800/50000]\n",
      "loss:0.814991 [46400/50000]\n",
      "loss:0.860785 [48000/50000]\n",
      "loss:0.571668 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.714700, Avg loss: 0.055403 \n",
      "\n",
      "Epoch 8\n",
      "--------\n",
      "loss:0.306574 [0/50000]\n",
      "loss:0.590932 [1600/50000]\n",
      "loss:0.565627 [3200/50000]\n",
      "loss:0.352821 [4800/50000]\n",
      "loss:0.619616 [6400/50000]\n",
      "loss:0.381452 [8000/50000]\n",
      "loss:0.261108 [9600/50000]\n",
      "loss:0.584343 [11200/50000]\n",
      "loss:0.464498 [12800/50000]\n",
      "loss:1.129250 [14400/50000]\n",
      "loss:0.838573 [16000/50000]\n",
      "loss:0.219870 [17600/50000]\n",
      "loss:0.362249 [19200/50000]\n",
      "loss:0.600708 [20800/50000]\n",
      "loss:0.807603 [22400/50000]\n",
      "loss:0.175068 [24000/50000]\n",
      "loss:0.270565 [25600/50000]\n",
      "loss:0.518466 [27200/50000]\n",
      "loss:0.590361 [28800/50000]\n",
      "loss:0.289672 [30400/50000]\n",
      "loss:0.545664 [32000/50000]\n",
      "loss:0.500196 [33600/50000]\n",
      "loss:0.762609 [35200/50000]\n",
      "loss:0.907233 [36800/50000]\n",
      "loss:0.278534 [38400/50000]\n",
      "loss:0.677141 [40000/50000]\n",
      "loss:0.616358 [41600/50000]\n",
      "loss:0.381726 [43200/50000]\n",
      "loss:0.840690 [44800/50000]\n",
      "loss:0.272671 [46400/50000]\n",
      "loss:0.181827 [48000/50000]\n",
      "loss:0.402613 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.712900, Avg loss: 0.055996 \n",
      "\n",
      "Epoch 9\n",
      "--------\n",
      "loss:0.686870 [0/50000]\n",
      "loss:0.243137 [1600/50000]\n",
      "loss:0.745182 [3200/50000]\n",
      "loss:0.199414 [4800/50000]\n",
      "loss:0.376802 [6400/50000]\n",
      "loss:0.977383 [8000/50000]\n",
      "loss:0.426316 [9600/50000]\n",
      "loss:0.857758 [11200/50000]\n",
      "loss:0.241814 [12800/50000]\n",
      "loss:0.438811 [14400/50000]\n",
      "loss:0.235140 [16000/50000]\n",
      "loss:0.840669 [17600/50000]\n",
      "loss:0.291790 [19200/50000]\n",
      "loss:0.305717 [20800/50000]\n",
      "loss:0.519503 [22400/50000]\n",
      "loss:0.624448 [24000/50000]\n",
      "loss:1.084359 [25600/50000]\n",
      "loss:0.787030 [27200/50000]\n",
      "loss:0.752135 [28800/50000]\n",
      "loss:0.204077 [30400/50000]\n",
      "loss:0.540441 [32000/50000]\n",
      "loss:0.240839 [33600/50000]\n",
      "loss:0.336970 [35200/50000]\n",
      "loss:0.212483 [36800/50000]\n",
      "loss:0.618625 [38400/50000]\n",
      "loss:0.815890 [40000/50000]\n",
      "loss:0.502705 [41600/50000]\n",
      "loss:0.678024 [43200/50000]\n",
      "loss:0.656776 [44800/50000]\n",
      "loss:0.290448 [46400/50000]\n",
      "loss:0.478707 [48000/50000]\n",
      "loss:0.588813 [49600/50000]\n",
      "Test Error: \n",
      " Accuracy: 0.721700, Avg loss: 0.055703 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8742726cd0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    print(\"--------\")\n",
    "    size_train_data = len(train_dataloader.dataset)\n",
    "    for num_batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_float = loss.item()\n",
    "        if num_batch % 100 == 0:\n",
    "            current = num_batch*batch_size\n",
    "            print(\"loss:%f [%d/%d]\" % (loss_float, current, size_train_data))\n",
    "        if (num_batch % 10 == 0):\n",
    "            losses.append(loss_float)\n",
    "    scheduler.step()\n",
    "\n",
    "    size_test_data = len(test_dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    for (x, y) in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        # pred = torch.softmax(pred, dim=1)\n",
    "        accuracy += (pred.argmax(dim=1) ==\n",
    "                     y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size_test_data\n",
    "    accuracy /= size_test_data\n",
    "    print(\"Test Error: \\n Accuracy: %f, Avg loss: %f \\n\" %\n",
    "          (accuracy, test_loss))\n",
    "\n",
    "plt.plot(np.squeeze(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "250fe364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
